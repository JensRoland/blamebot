#!/usr/bin/env python3
"""
blamebot: understand why AI-authored code exists.

Usage:
    git-blamebot <file>                    # all reasons for a file
    git-blamebot -L <line> <file>          # reasons for a specific line
    git-blamebot -L <start>:<end> <file>   # reasons for a line range
    git-blamebot --since <date> [<file>]   # reasons since a date
    git-blamebot --grep <text>             # search prompts & changes
    git-blamebot --author <name>           # filter by author
    git-blamebot --trace <id>              # show reasoning from transcript
    git-blamebot --explain <file> [-L N]   # deep explanation using Sonnet
    git-blamebot --stats                   # summary statistics
    git-blamebot --json                    # machine-readable JSON output
    git-blamebot --rebuild                 # force index rebuild
    git-blamebot --log [--hook]            # show debug logs
    git-blamebot --dump-payload            # show last raw hook payload
    git-blamebot --uninit                  # remove tracking from this repo
"""

import argparse
import difflib
import sqlite3
import subprocess
import sys
import json
import hashlib
import os
import re
import shutil
import textwrap
from pathlib import Path


# ── Index management ─────────────────────────────────────────────────

def get_project_root() -> Path:
    result = subprocess.run(
        ["git", "rev-parse", "--show-toplevel"],
        capture_output=True, text=True
    )
    if result.returncode != 0:
        print("Error: not inside a git repository", file=sys.stderr)
        sys.exit(1)
    return Path(result.stdout.strip())


def get_paths(project_root: Path) -> dict:
    return {
        "log_dir": project_root / ".blamebot" / "log",
        "traces_dir": project_root / ".blamebot" / "traces",
        "cache_dir": project_root / ".git" / "blamebot",
        "index_db": project_root / ".git" / "blamebot" / "index.db",
    }


def index_is_stale(paths: dict) -> bool:
    index_db = paths["index_db"]
    log_dir = paths["log_dir"]

    if not index_db.exists():
        return True
    if not log_dir.exists():
        return False

    index_mtime = index_db.stat().st_mtime
    for jsonl in log_dir.glob("*.jsonl"):
        if jsonl.stat().st_mtime > index_mtime:
            return True
    return False


def rebuild_index(paths: dict, quiet: bool = False) -> sqlite3.Connection:
    index_db = paths["index_db"]
    log_dir = paths["log_dir"]

    paths["cache_dir"].mkdir(parents=True, exist_ok=True)

    if index_db.exists():
        index_db.unlink()

    conn = sqlite3.connect(str(index_db))
    conn.row_factory = sqlite3.Row
    conn.execute("""
        CREATE TABLE reasons (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file TEXT NOT NULL,
            line_start INTEGER,
            line_end INTEGER,
            content_hash TEXT,
            ts TEXT NOT NULL,
            prompt TEXT,
            reason TEXT,
            change TEXT,
            tool TEXT,
            author TEXT,
            session TEXT,
            trace TEXT,
            source_file TEXT
        )
    """)
    conn.execute("CREATE INDEX idx_file ON reasons(file)")
    conn.execute("CREATE INDEX idx_content_hash ON reasons(content_hash)")
    conn.execute("CREATE INDEX idx_ts ON reasons(ts)")
    conn.execute("CREATE INDEX idx_author ON reasons(author)")

    record_count = 0
    file_count = 0

    if log_dir.exists():
        for jsonl_path in sorted(log_dir.glob("*.jsonl")):
            file_count += 1
            with open(jsonl_path) as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        rec = json.loads(line)
                    except json.JSONDecodeError:
                        continue

                    lines = rec.get("lines", [None, None])
                    conn.execute("""
                        INSERT INTO reasons
                        (file, line_start, line_end, content_hash, ts,
                         prompt, reason, change, tool, author, session, trace,
                         source_file)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        rec.get("file", ""),
                        lines[0] if lines else None,
                        lines[1] if len(lines) > 1 else None,
                        rec.get("content_hash", ""),
                        rec.get("ts", ""),
                        rec.get("prompt", ""),
                        rec.get("reason", ""),
                        rec.get("change", rec.get("reason", "")),
                        rec.get("tool", ""),
                        rec.get("author", ""),
                        rec.get("session", ""),
                        rec.get("trace", ""),
                        jsonl_path.name,
                    ))
                    record_count += 1

    conn.commit()

    if not quiet:
        print(f"{DIM}Index rebuilt: {record_count} records from {file_count} log files{RESET}\n",
              file=sys.stderr)

    return conn


def get_db(paths: dict, force_rebuild: bool = False) -> sqlite3.Connection:
    if force_rebuild or index_is_stale(paths):
        return rebuild_index(paths)
    conn = sqlite3.connect(str(paths["index_db"]))
    conn.row_factory = sqlite3.Row
    return conn


# ── Git blame cross-reference ────────────────────────────────────────

def get_blame_for_line(project_root: Path, filepath: str, line: int) -> dict | None:
    try:
        result = subprocess.run(
            ["git", "blame", "-L", f"{line},{line}", "--porcelain", filepath],
            capture_output=True, text=True,
            cwd=str(project_root),
        )
        if result.returncode != 0:
            return None

        info = {}
        for bline in result.stdout.splitlines():
            if bline.startswith("author "):
                info["blame_author"] = bline[7:]
            elif bline.startswith("summary "):
                info["blame_summary"] = bline[8:]
            elif not info and " " in bline:
                parts = bline.split()
                if len(parts) >= 1 and len(parts[0]) == 40:
                    info["blame_sha"] = parts[0]
        return info if info else None
    except Exception:
        return None


# ── Content hash matching ────────────────────────────────────────────

def current_content_hash(filepath: Path, line_start: int, line_end: int) -> str:
    if not filepath.exists():
        return ""
    try:
        lines = filepath.read_text().splitlines()
        if line_start and line_end:
            region = "\n".join(lines[line_start - 1:line_end])
        else:
            region = "\n".join(lines)
        normalized = " ".join(region.split())
        return hashlib.sha256(normalized.encode()).hexdigest()[:16]
    except Exception:
        return ""


# ── Transcript reader ────────────────────────────────────────────────

def read_trace_context(trace_ref: str, project_root: Path = None) -> str | None:
    """
    Read reasoning context for a specific tool call.
    trace_ref format: /path/to/transcript.jsonl#tool_use_id

    Lookup order:
    1. Committed traces in .blamebot/traces/<session_id>.json (works for everyone)
    2. Full local transcript file (works for original dev only)
    """
    if not trace_ref:
        return None

    parts = trace_ref.split("#", 1)
    transcript_path = parts[0]
    tool_use_id = parts[1] if len(parts) > 1 else None

    # 1. Try committed traces (portable, works for all team members)
    if project_root and tool_use_id and transcript_path:
        session_id = Path(transcript_path).stem
        traces_file = project_root / ".blamebot" / "traces" / f"{session_id}.json"
        if traces_file.exists():
            try:
                traces = json.loads(traces_file.read_text())
                context = traces.get(tool_use_id)
                if context:
                    return context
            except (json.JSONDecodeError, OSError):
                pass

    # 2. Fall back to full local transcript
    if not transcript_path or not Path(transcript_path).exists():
        return None

    try:
        entries = []
        with open(transcript_path) as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        entries.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue

        if not tool_use_id:
            return f"Transcript has {len(entries)} entries (no tool_use_id to locate specific call)"

        # Find the entry containing this tool_use_id
        target_idx = None
        for i, entry in enumerate(entries):
            message = entry.get("message", {})
            content = message.get("content", [])
            if not isinstance(content, list):
                continue
            for block in content:
                if isinstance(block, dict) and block.get("id") == tool_use_id:
                    target_idx = i
                    break
            if target_idx is not None:
                break

        if target_idx is None:
            return f"tool_use_id {tool_use_id} not found in transcript ({len(entries)} entries)"

        # Walk backwards from the tool_use entry to find thinking/text.
        # Skip over tool_result (user) entries and other tool_use (assistant)
        # entries — these are intermediate tool calls between the reasoning
        # and our target edit.
        context_parts = []
        found_reasoning = False
        for j in range(target_idx - 1, -1, -1):
            prev = entries[j]
            msg = prev.get("message", {})
            role = msg.get("role", prev.get("type", ""))

            content = msg.get("content", [])
            if not isinstance(content, list):
                continue

            # Skip tool_result entries (user role returning tool output)
            if role == "user":
                is_tool_result = all(
                    isinstance(b, dict) and b.get("type") == "tool_result"
                    for b in content if isinstance(b, dict)
                )
                if is_tool_result:
                    continue
                break  # real user message — stop

            if role != "assistant":
                continue

            for block in content:
                if not isinstance(block, dict):
                    continue
                if block.get("type") == "thinking":
                    thinking = block.get("thinking", "")
                    if thinking:
                        context_parts.append(f"[Thinking]\n{thinking[-1500:]}")
                        found_reasoning = True
                elif block.get("type") == "text":
                    text = block.get("text", "")
                    if text:
                        context_parts.append(f"[Response]\n{text[-500:]}")
                        found_reasoning = True
                elif block.get("type") == "tool_use":
                    # Skip other tool calls (Read, Glob, etc.)
                    continue

            if found_reasoning:
                break

        if context_parts:
            # Reverse so thinking comes before text (chronological order)
            context_parts.reverse()
            return "\n\n".join(context_parts)
        else:
            return f"Tool call found at entry {target_idx}, but no thinking/text blocks found before it"

    except Exception as e:
        return f"Error reading transcript: {e}"


def extract_diff_from_trace(trace_ref: str) -> tuple[str, str] | None:
    """Extract old_string/new_string from the local transcript for a tool call."""
    if not trace_ref:
        return None

    parts = trace_ref.split("#", 1)
    transcript_path = parts[0]
    tool_use_id = parts[1] if len(parts) > 1 else None

    if not tool_use_id or not transcript_path or not Path(transcript_path).exists():
        return None

    try:
        with open(transcript_path) as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    entry = json.loads(line)
                except json.JSONDecodeError:
                    continue

                message = entry.get("message", {})
                content = message.get("content", [])
                if not isinstance(content, list):
                    continue
                for block in content:
                    if isinstance(block, dict) and block.get("id") == tool_use_id:
                        inp = block.get("input", {})
                        old_str = inp.get("old_string")
                        new_str = inp.get("new_string")
                        if old_str is not None or new_str is not None:
                            return (old_str or "", new_str or "")
                        # Write tool: content is the new file
                        file_content = inp.get("content", "")
                        if file_content:
                            return ("", file_content)
                        return None
    except Exception:
        pass

    return None


def extract_trace_contexts(transcript_path: str, tool_use_ids: list[str]) -> dict[str, str]:
    """
    Extract trace context for multiple tool_use_ids from a single transcript.
    Returns {tool_use_id: context_string} for each ID that has reasoning.
    """
    if not tool_use_ids or not transcript_path or not Path(transcript_path).exists():
        return {}

    try:
        entries = []
        with open(transcript_path) as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        entries.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue
    except Exception:
        return {}

    # Build index: tool_use_id -> entry index
    id_to_idx: dict[str, int] = {}
    for i, entry in enumerate(entries):
        message = entry.get("message", {})
        content = message.get("content", [])
        if not isinstance(content, list):
            continue
        for block in content:
            if isinstance(block, dict) and block.get("id") in tool_use_ids:
                id_to_idx[block["id"]] = i

    results = {}
    for tool_use_id in tool_use_ids:
        target_idx = id_to_idx.get(tool_use_id)
        if target_idx is None:
            continue

        # Same backwards walk as read_trace_context
        context_parts = []
        found_reasoning = False
        for j in range(target_idx - 1, -1, -1):
            prev = entries[j]
            msg = prev.get("message", {})
            role = msg.get("role", prev.get("type", ""))

            content = msg.get("content", [])
            if not isinstance(content, list):
                continue

            if role == "user":
                is_tool_result = all(
                    isinstance(b, dict) and b.get("type") == "tool_result"
                    for b in content if isinstance(b, dict)
                )
                if is_tool_result:
                    continue
                break

            if role != "assistant":
                continue

            for block in content:
                if not isinstance(block, dict):
                    continue
                if block.get("type") == "thinking":
                    thinking = block.get("thinking", "")
                    if thinking:
                        context_parts.append(f"[Thinking]\n{thinking[-1500:]}")
                        found_reasoning = True
                elif block.get("type") == "text":
                    text = block.get("text", "")
                    if text:
                        context_parts.append(f"[Response]\n{text[-500:]}")
                        found_reasoning = True
                elif block.get("type") == "tool_use":
                    continue

            if found_reasoning:
                break

        if context_parts:
            context_parts.reverse()
            results[tool_use_id] = "\n\n".join(context_parts)

    return results


# ── Formatters ───────────────────────────────────────────────────────

RESET = "\033[0m"
BOLD = "\033[1m"
DIM = "\033[2m"
YELLOW = "\033[33m"
CYAN = "\033[36m"
GREEN = "\033[32m"
MAGENTA = "\033[35m"
BLUE = "\033[34m"
RED = "\033[31m"


def row_to_dict(row, project_root: Path) -> dict:
    """Convert a sqlite3.Row to a JSON-serializable dict."""
    d = {
        "file": row["file"],
        "lines": [row["line_start"], row["line_end"]],
        "ts": row["ts"],
        "prompt": row["prompt"] or "",
        "reason": row["reason"] or "",
        "change": row["change"] or "",
        "tool": row["tool"] or "",
        "author": row["author"] or "",
        "content_hash": row["content_hash"] or "",
        "session": row["session"] or "",
        "trace": row["trace"] or "",
        "source_file": row["source_file"] or "",
    }
    # Content hash match status
    if row["content_hash"] and row["line_start"]:
        fp = project_root / row["file"]
        current = current_content_hash(fp, row["line_start"],
                                       row["line_end"] or row["line_start"])
        if current == row["content_hash"]:
            d["match"] = "exact"
        elif current:
            d["match"] = "changed"
        else:
            d["match"] = "unknown"
    return d


def format_side_by_side_diff(old_text: str, new_text: str) -> str:
    """Render a side-by-side diff with box-drawing borders."""
    try:
        term_width = os.get_terminal_size().columns
    except (AttributeError, ValueError, OSError):
        term_width = 80

    # Layout: │ <left> │ <right> │  →  total = 2*col_w + 7
    col_w = max((term_width - 7) // 2, 20)

    old_lines = (old_text or "").expandtabs(4).splitlines() or [""]
    new_lines = (new_text or "").expandtabs(4).splitlines() or [""]

    sm = difflib.SequenceMatcher(None, old_lines, new_lines)

    rows = []
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag == "equal":
            for i, j in zip(range(i1, i2), range(j1, j2)):
                rows.append(("equal", old_lines[i], new_lines[j]))
        elif tag == "replace":
            old_chunk = old_lines[i1:i2]
            new_chunk = new_lines[j1:j2]
            for k in range(max(len(old_chunk), len(new_chunk))):
                o = old_chunk[k] if k < len(old_chunk) else None
                n = new_chunk[k] if k < len(new_chunk) else None
                rows.append(("replace", o, n))
        elif tag == "delete":
            for i in range(i1, i2):
                rows.append(("delete", old_lines[i], None))
        elif tag == "insert":
            for j in range(j1, j2):
                rows.append(("insert", None, new_lines[j]))

    total_rows = len(rows)
    max_display = 40
    truncated = total_rows > max_display
    if truncated:
        rows = rows[:max_display]

    output = []

    # Top border with labels
    lbl_l = "─ Before "
    lbl_r = "─ After "
    output.append(
        f"┌{lbl_l}{'─' * (col_w + 2 - len(lbl_l))}"
        f"┬{lbl_r}{'─' * (col_w + 2 - len(lbl_r))}┐"
    )

    for tag, old_line, new_line in rows:
        left = (old_line or "")[:col_w].ljust(col_w)
        right = (new_line or "")[:col_w].ljust(col_w)

        if tag == "equal":
            output.append(f"│ {DIM}{left}{RESET} │ {DIM}{right}{RESET} │")
        elif tag == "delete":
            output.append(f"│ {RED}{left}{RESET} │ {' ' * col_w} │")
        elif tag == "insert":
            output.append(f"│ {' ' * col_w} │ {GREEN}{right}{RESET} │")
        elif tag == "replace":
            l = f"{RED}{left}{RESET}" if old_line is not None else " " * col_w
            r = f"{GREEN}{right}{RESET}" if new_line is not None else " " * col_w
            output.append(f"│ {l} │ {r} │")

    # Bottom border
    output.append(f"└{'─' * (col_w + 2)}┴{'─' * (col_w + 2)}┘")

    if truncated:
        output.append(f"  {DIM}… {total_rows - max_display} more lines not shown{RESET}")

    return "\n".join(output)


def format_bordered_text(text: str, title: str = "") -> str:
    """Render text inside a bordered box with word-wrapped lines."""
    try:
        term_width = os.get_terminal_size().columns
    except (AttributeError, ValueError, OSError):
        term_width = 80

    # │ <content> │  →  inner_w + 4
    inner_w = max(term_width - 4, 30)

    wrapped = []
    for paragraph in text.split("\n"):
        if not paragraph.strip():
            wrapped.append("")
        else:
            wrapped.extend(textwrap.wrap(paragraph, width=inner_w))

    output = []

    # Top border with optional title
    if title:
        lbl = f"─ {title} "
        output.append(f"┌{lbl}{'─' * (inner_w + 2 - len(lbl))}┐")
    else:
        output.append(f"┌{'─' * (inner_w + 2)}┐")

    for line in wrapped:
        padded = line[:inner_w].ljust(inner_w)
        output.append(f"│ {padded} │")

    output.append(f"└{'─' * (inner_w + 2)}┘")

    return "\n".join(output)


def format_reason(row, project_root: Path, verbose: bool = False):
    ts = row["ts"] or "?"
    date = ts[:10] if len(ts) >= 10 else ts

    prompt = row["prompt"] or ""
    reason = row["reason"] or ""
    change = row["change"] or ""
    file = row["file"] or "?"
    author = row["author"] or ""

    lines = ""
    if row["line_start"]:
        if row["line_end"] and row["line_end"] != row["line_start"]:
            lines = f"L{row['line_start']}-{row['line_end']}"
        else:
            lines = f"L{row['line_start']}"

    # Content hash match check
    match_indicator = ""
    if row["content_hash"] and row["line_start"]:
        fp = project_root / row["file"]
        current = current_content_hash(fp, row["line_start"],
                                       row["line_end"] or row["line_start"])
        if current == row["content_hash"]:
            match_indicator = f" {GREEN}✓{RESET}"
        elif current:
            match_indicator = f" {YELLOW}~{RESET}"

    header = f"{CYAN}{date}{RESET}"
    if author:
        header += f" {BLUE}{author}{RESET}"
    header += f"  {BOLD}{file}{RESET}"
    if lines:
        header += f" {DIM}{lines}{RESET}"
    header += match_indicator

    parts = [header]

    if prompt:
        prompt_display = prompt if len(prompt) <= 120 else prompt[:117] + "..."
        parts.append(f"  {YELLOW}Prompt:{RESET} {prompt_display}")

    if reason:
        reason_display = reason if len(reason) <= 200 else reason[:197] + "..."
        parts.append(f"  {MAGENTA}Reason:{RESET} {reason_display}")
    elif change:
        # Fall back to diff summary if no transcript reason available
        parts.append(f"  {MAGENTA}Change:{RESET} {change}")

    if verbose:
        if reason and change:
            parts.append(f"  {DIM}Diff:    {change}{RESET}")
        if row["tool"]:
            parts.append(f"  {DIM}Tool:    {row['tool']}{RESET}")
        if row["content_hash"]:
            parts.append(f"  {DIM}Hash:    {row['content_hash']}{RESET}")
        if row["session"]:
            parts.append(f"  {DIM}Session: {row['session'][:36]}{RESET}")
        if row["trace"]:
            # Show just the tool_use_id part for readability
            trace = row["trace"]
            if "#" in trace:
                parts.append(f"  {DIM}Trace:   #{trace.split('#', 1)[1]}{RESET}")
            else:
                parts.append(f"  {DIM}Trace:   {trace[-60:]}{RESET}")
        if row["source_file"]:
            parts.append(f"  {DIM}Source:  {row['source_file']}{RESET}")

        # Git blame cross-reference
        if row["line_start"]:
            blame = get_blame_for_line(project_root, row["file"], row["line_start"])
            if blame and blame.get("blame_sha"):
                sha_short = blame["blame_sha"][:8]
                summary = blame.get("blame_summary", "")
                parts.append(f"  {DIM}Commit:  {sha_short} {summary}{RESET}")

    return "\n".join(parts)


# ── Commands ─────────────────────────────────────────────────────────

def relative_path(filepath: str, project_root: Path) -> str:
    p = Path(filepath)
    if p.is_absolute():
        try:
            return str(p.relative_to(project_root))
        except ValueError:
            return filepath
    return filepath


def cmd_file(conn, filepath: str, project_root: Path, line=None, verbose=False,
             json_output=False):
    rel = relative_path(filepath, project_root)

    conditions = ["(file = ? OR file LIKE ?)"]
    params = [rel, f"%/{rel}"]

    if line:
        if ":" in line:
            start, end = line.split(":", 1)
            conditions.append("(line_start <= ? AND (line_end >= ? OR line_end IS NULL))")
            params.extend([int(end), int(start)])
        else:
            line_num = int(line)
            conditions.append(
                "(line_start <= ? AND (line_end >= ? OR line_start = ?))"
            )
            params.extend([line_num, line_num, line_num])

    where = " AND ".join(conditions)
    limit = " LIMIT 1" if line else ""
    rows = conn.execute(
        f"SELECT * FROM reasons WHERE {where} ORDER BY ts DESC{limit}", params
    ).fetchall()

    if json_output:
        print(json.dumps([row_to_dict(r, project_root) for r in rows], indent=2))
        return

    if not rows:
        print(f"No reasons found for {rel}" + (f" at line {line}" if line else ""))
        return

    for row in rows:
        print(format_reason(row, project_root, verbose))
        print()


def cmd_grep(conn, pattern: str, project_root: Path, verbose=False,
             json_output=False):
    rows = conn.execute("""
        SELECT * FROM reasons
        WHERE prompt LIKE ? OR reason LIKE ? OR change LIKE ?
        ORDER BY ts DESC
    """, (f"%{pattern}%", f"%{pattern}%", f"%{pattern}%")).fetchall()

    if json_output:
        print(json.dumps([row_to_dict(r, project_root) for r in rows], indent=2))
        return

    if not rows:
        print(f"No reasons matching '{pattern}'")
        return

    print(f"Found {len(rows)} reason(s) matching '{pattern}':\n")
    for row in rows:
        print(format_reason(row, project_root, verbose))
        print()


def cmd_since(conn, date_str: str, filepath, project_root: Path, verbose=False,
              json_output=False):
    conditions = ["ts >= ?"]
    params = [date_str]

    if filepath:
        rel = relative_path(filepath, project_root)
        conditions.append("(file = ? OR file LIKE ?)")
        params.extend([rel, f"%/{rel}"])

    where = " AND ".join(conditions)
    rows = conn.execute(
        f"SELECT * FROM reasons WHERE {where} ORDER BY ts DESC", params
    ).fetchall()

    if json_output:
        print(json.dumps([row_to_dict(r, project_root) for r in rows], indent=2))
        return

    if not rows:
        print(f"No reasons found since {date_str}")
        return

    print(f"Found {len(rows)} reason(s) since {date_str}:\n")
    for row in rows:
        print(format_reason(row, project_root, verbose))
        print()


def cmd_author(conn, author: str, project_root: Path, verbose=False,
               json_output=False):
    rows = conn.execute(
        "SELECT * FROM reasons WHERE author LIKE ? ORDER BY ts DESC",
        (f"%{author}%",)
    ).fetchall()

    if json_output:
        print(json.dumps([row_to_dict(r, project_root) for r in rows], indent=2))
        return

    if not rows:
        print(f"No reasons found for author '{author}'")
        return

    print(f"Found {len(rows)} reason(s) by '{author}':\n")
    for row in rows:
        print(format_reason(row, project_root, verbose))
        print()


def cmd_trace(conn, record_id: str, project_root: Path, json_output=False):
    """Show the reasoning trace for a specific record."""
    rows = conn.execute("SELECT * FROM reasons WHERE id = ?", (record_id,)).fetchall()

    if not rows:
        # Try matching by tool_use_id fragment
        rows = conn.execute(
            "SELECT * FROM reasons WHERE trace LIKE ?",
            (f"%{record_id}%",)
        ).fetchall()

    if not rows:
        if json_output:
            print("[]")
        else:
            print(f"No record found for '{record_id}'")
        return

    row = rows[0]

    if json_output:
        d = row_to_dict(row, project_root)
        trace_ref = row["trace"]
        if trace_ref:
            context = read_trace_context(trace_ref, project_root)
            if context:
                d["trace_context"] = context
        print(json.dumps([d], indent=2))
        return

    print(format_reason(row, project_root, verbose=True))
    print()

    trace_ref = row["trace"]
    if not trace_ref:
        print(f"{YELLOW}No trace reference stored for this record.{RESET}")
        return

    print(f"{BOLD}Reasoning trace:{RESET}\n")
    context = read_trace_context(trace_ref, project_root)
    if context:
        print(context)
    else:
        print(f"{DIM}Transcript not accessible (may be on another machine).{RESET}")
        print(f"{DIM}Trace ref: {trace_ref}{RESET}")


def cmd_explain(conn, filepath: str, project_root: Path, line=None):
    """Generate a thorough explanation of why an edit was made, using Sonnet."""
    rel = relative_path(filepath, project_root)

    conditions = ["(file = ? OR file LIKE ?)"]
    params = [rel, f"%/{rel}"]

    if line:
        if ":" in line:
            start, end = line.split(":", 1)
            conditions.append("(line_start <= ? AND (line_end >= ? OR line_end IS NULL))")
            params.extend([int(end), int(start)])
        else:
            line_num = int(line)
            conditions.append(
                "(line_start <= ? AND (line_end >= ? OR line_start = ?))"
            )
            params.extend([line_num, line_num, line_num])

    where = " AND ".join(conditions)
    rows = conn.execute(
        f"SELECT * FROM reasons WHERE {where} ORDER BY ts DESC", params
    ).fetchall()

    if not rows:
        loc = f" at line {line}" if line else ""
        print(f"No reasons found for {rel}{loc}")
        return

    if len(rows) > 1:
        print(f"{DIM}({len(rows)} records match — explaining the most recent){RESET}\n")

    row = rows[0]
    print(format_reason(row, project_root, verbose=False))
    print()

    # Show side-by-side diff if available
    diff_data = extract_diff_from_trace(row["trace"])
    if diff_data:
        old_str, new_str = diff_data
        # Skip display for very large changes (e.g. full file writes)
        if (old_str or new_str) and old_str.count("\n") + new_str.count("\n") < 200:
            print(format_side_by_side_diff(old_str, new_str))
            print()
    elif row["change"]:
        # Fall back: parse the compact change summary
        change = row["change"]
        if " → " in change:
            left, _, right = change.partition(" → ")
            left = left.lstrip("…").strip()
            right = right.lstrip("…").strip()
            if left or right:
                print(format_side_by_side_diff(left, right))
                print()
        elif change.startswith("added: "):
            print(format_side_by_side_diff("", change[7:]))
            print()
        elif change.startswith("removed: "):
            print(format_side_by_side_diff(change[9:], ""))
            print()

    # Gather context
    trace_ref = row["trace"]
    trace_context = read_trace_context(trace_ref, project_root) if trace_ref else None

    # Get session prompts — from transcript or from sibling records
    session_prompts = []
    if trace_ref:
        transcript_path = trace_ref.split("#", 1)[0]
        session_prompts = extract_session_prompts(transcript_path)

    if not session_prompts:
        # Fall back: gather prompts from same session in the index
        session = row["session"]
        if session:
            sibling_rows = conn.execute(
                "SELECT DISTINCT prompt FROM reasons WHERE session = ? AND prompt != '' ORDER BY ts",
                (session,)
            ).fetchall()
            session_prompts = [r["prompt"] for r in sibling_rows]

    # Build the explain prompt
    parts = [
        "You are explaining why a specific AI-authored code edit was made.",
        "Given the context below, write a clear, thorough explanation (2-5 sentences)",
        "of WHY this change was made — the user's intent, the reasoning, and how",
        "this edit fits into the broader task. Write in third person past tense.",
        "",
    ]

    if session_prompts:
        parts.append("Session prompt history (in order):")
        for i, p in enumerate(session_prompts, 1):
            display = p if len(p) <= 300 else p[:297] + "..."
            parts.append(f'{i}. "{display}"')
        parts.append("")

    parts.append(f"File: {row['file']}")
    if row["line_start"]:
        if row["line_end"] and row["line_end"] != row["line_start"]:
            parts.append(f"Lines: {row['line_start']}-{row['line_end']}")
        else:
            parts.append(f"Line: {row['line_start']}")

    if row["change"]:
        parts.append(f"Change: {row['change']}")

    if diff_data:
        old_str, new_str = diff_data
        if old_str:
            parts.append(f"\nOriginal code:\n{old_str[:2000]}")
        if new_str:
            parts.append(f"\nNew code:\n{new_str[:2000]}")

    if row["reason"]:
        parts.append(f"One-line reason: {row['reason']}")

    if trace_context:
        parts.append(f"\nAgent's internal reasoning:\n{trace_context}")

    parts.append("")
    parts.append("Write the explanation as plain text, no markdown or formatting.")

    prompt = "\n".join(parts)

    print(f"{DIM}(calling Sonnet...){RESET}", file=sys.stderr, end="", flush=True)

    explanation = call_llm(prompt, model="claude-sonnet-4-6", timeout=90)
    print(f"\r{' ' * 30}\r", file=sys.stderr, end="", flush=True)

    if explanation:
        print(format_bordered_text(explanation, "Explanation"))
    else:
        print("Failed to generate explanation.", file=sys.stderr)


def cmd_stats(conn, json_output=False):
    total = conn.execute("SELECT COUNT(*) FROM reasons").fetchone()[0]
    files = conn.execute("SELECT COUNT(DISTINCT file) FROM reasons").fetchone()[0]
    authors = conn.execute("SELECT COUNT(DISTINCT author) FROM reasons").fetchone()[0]
    sources = conn.execute("SELECT COUNT(DISTINCT source_file) FROM reasons").fetchone()[0]

    first = conn.execute("SELECT MIN(ts) FROM reasons").fetchone()[0]
    last = conn.execute("SELECT MAX(ts) FROM reasons").fetchone()[0]

    top_files = conn.execute("""
        SELECT file, COUNT(*) as cnt FROM reasons
        GROUP BY file ORDER BY cnt DESC LIMIT 5
    """).fetchall()

    top_authors = conn.execute("""
        SELECT author, COUNT(*) as cnt FROM reasons
        WHERE author != '' GROUP BY author ORDER BY cnt DESC LIMIT 5
    """).fetchall()

    if json_output:
        print(json.dumps({
            "total_records": total,
            "files_tracked": files,
            "authors": authors,
            "log_files": sources,
            "first_record": first,
            "last_record": last,
            "top_files": [{"file": r["file"], "count": r["cnt"]} for r in top_files],
            "top_authors": [{"author": r["author"], "count": r["cnt"]} for r in top_authors],
        }, indent=2))
        return

    print(f"{BOLD}blamebot statistics{RESET}\n")
    print(f"  Total records:  {total}")
    print(f"  Files tracked:  {files}")
    print(f"  Authors:        {authors}")
    print(f"  Log files:      {sources}")
    print(f"  First record:   {first or 'n/a'}")
    print(f"  Last record:    {last or 'n/a'}")

    if top_files:
        print(f"\n  {BOLD}Most edited files:{RESET}")
        for row in top_files:
            print(f"    {row['cnt']:4d}  {row['file']}")

    if top_authors:
        print(f"\n  {BOLD}By author:{RESET}")
        for row in top_authors:
            print(f"    {row['cnt']:4d}  {row['author']}")


def cmd_log(cache_dir: Path, hook_log: bool = False):
    log_name = "hook.log" if hook_log else "capture_prompt.log"
    log_file = cache_dir / "logs" / log_name

    if not log_file.exists():
        print(f"No log file at {log_file}")
        return

    content = log_file.read_text()
    lines = content.splitlines()
    tail = lines[-100:] if len(lines) > 100 else lines
    print(f"{DIM}--- {log_file} (last {len(tail)} lines) ---{RESET}\n")
    print("\n".join(tail))


def cmd_dump_payload(cache_dir: Path):
    for log_name in ["hook.log", "capture_prompt.log"]:
        log_file = cache_dir / "logs" / log_name
        if log_file.exists():
            print(f"\n{BOLD}=== {log_name} (last entry) ==={RESET}\n")
            content = log_file.read_text()
            entries = content.split("=" * 60)
            for entry in entries[-3:]:
                if entry.strip():
                    print(entry.strip())
                    print()


# ── Fill reasons (LLM) ────────────────────────────────────────────────

def clean_prompt(raw: str) -> str:
    """Strip IDE metadata tags, keeping only the user's actual request."""
    cleaned = re.sub(r'<ide_\w+>.*?</ide_\w+>\s*', '', raw, flags=re.DOTALL)
    cleaned = re.sub(r'<system-reminder>.*?</system-reminder>\s*', '', cleaned, flags=re.DOTALL)
    return cleaned.strip()


def extract_session_prompts(transcript_path: str) -> list[str]:
    """Extract the ordered list of user prompts from a transcript."""
    path = Path(transcript_path)
    if not path.exists():
        return []

    prompts = []
    try:
        for line in path.read_text().splitlines():
            line = line.strip()
            if not line:
                continue
            try:
                entry = json.loads(line)
            except json.JSONDecodeError:
                continue

            message = entry.get("message", {})
            if message.get("role") != "user":
                continue

            content = message.get("content", [])
            if not isinstance(content, list):
                continue

            # Skip tool_result entries
            if all(
                isinstance(b, dict) and b.get("type") == "tool_result"
                for b in content if isinstance(b, dict)
            ):
                continue

            # Extract text blocks
            for block in content:
                if isinstance(block, dict) and block.get("type") == "text":
                    text = clean_prompt(block.get("text", ""))
                    if text:
                        prompts.append(text)
    except Exception:
        pass

    return prompts


def build_fill_prompt(session_prompts: list[str], edits: list[dict]) -> str:
    """Build the prompt to send to Haiku for reason generation."""
    parts = [
        "You are generating concise reasons for AI code edits.",
        "Given the session prompt history and edit details below,",
        "write a brief reason (1 sentence max) for each edit",
        "explaining WHY the change was made.",
        "",
        "Session prompt history (in order):",
    ]
    for i, p in enumerate(session_prompts, 1):
        # Truncate very long prompts
        display = p if len(p) <= 200 else p[:197] + "..."
        parts.append(f'{i}. "{display}"')

    parts.append("")
    parts.append("Edits:")
    for edit in edits:
        lines = ""
        if edit.get("line_start"):
            if edit.get("line_end") and edit["line_end"] != edit["line_start"]:
                lines = f" L{edit['line_start']}-{edit['line_end']}"
            else:
                lines = f" L{edit['line_start']}"

        parts.append(f"[{edit['id']}] File: {edit['file']}{lines}")
        parts.append(f"    Change: {edit['change']}")

    parts.append("")
    parts.append('Respond with ONLY a JSON array: [{"id": 1, "reason": "..."}, ...]')
    return "\n".join(parts)


def call_llm(prompt: str, model: str = "claude-haiku-4-5-20251001",
             timeout: int = 60) -> str | None:
    """Call a Claude model and return the raw text response."""
    env = {k: v for k, v in os.environ.items() if k != "CLAUDECODE"}
    try:
        result = subprocess.run(
            ["claude", "-p", "--model", model,
             "--output-format", "text", "--allowed-tools", ""],
            input=prompt, capture_output=True, text=True, env=env,
            timeout=timeout,
        )
    except FileNotFoundError:
        print("Error: 'claude' CLI not found on PATH", file=sys.stderr)
        return None
    except subprocess.TimeoutExpired:
        print("Error: LLM call timed out", file=sys.stderr)
        return None

    if result.returncode != 0:
        print(f"Error: claude CLI failed: {result.stderr[:200]}", file=sys.stderr)
        return None

    return result.stdout.strip() or None


def call_haiku(prompt: str) -> list[dict] | None:
    """Call Claude Haiku and parse a JSON array response."""
    text = call_llm(prompt)
    if not text:
        return None

    # Strip markdown code fences
    text = re.sub(r'^```json\s*', '', text)
    text = re.sub(r'\s*```$', '', text)

    try:
        data = json.loads(text)
        if isinstance(data, list):
            return data
    except json.JSONDecodeError:
        pass

    print(f"Error: could not parse Haiku response as JSON:\n{text[:300]}", file=sys.stderr)
    return None


def cmd_fill_reasons(paths: dict, project_root: Path, dry_run: bool):
    """Fill empty reasons in staged JSONL files using Claude Haiku."""
    log_dir = paths["log_dir"]

    # Find staged .blamebot/log/*.jsonl files
    result = subprocess.run(
        ["git", "diff", "--cached", "--name-only", "--", ".blamebot/log/*.jsonl"],
        capture_output=True, text=True, cwd=str(project_root),
    )
    staged_files = [f.strip() for f in result.stdout.splitlines() if f.strip()]

    if not staged_files:
        print("No staged .blamebot/log/*.jsonl files found.", file=sys.stderr)
        return

    # Read all records from staged files, track which need filling
    file_records: dict[str, list[dict]] = {}  # filename -> list of parsed records
    needs_fill = 0

    for rel_path in staged_files:
        abs_path = project_root / rel_path
        if not abs_path.exists():
            continue

        records = []
        with open(abs_path) as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    records.append(json.loads(line))
                except json.JSONDecodeError:
                    records.append(None)  # preserve line position

        file_records[rel_path] = records
        needs_fill += sum(1 for r in records if r and not r.get("reason"))

    if needs_fill == 0:
        print("All records already have reasons.", file=sys.stderr)
        return

    print(f"Found {needs_fill} record(s) to fill across {len(staged_files)} file(s).",
          file=sys.stderr)

    # Group records by transcript path for batching
    transcript_groups: dict[str, list[dict]] = {}
    edit_id = 0
    for rel_path, records in file_records.items():
        for i, rec in enumerate(records):
            if not rec or rec.get("reason"):
                continue
            trace = rec.get("trace", "")
            transcript = trace.split("#", 1)[0] if "#" in trace else ""
            if not transcript:
                continue
            edit_id += 1
            entry = {
                "id": edit_id,
                "file": rec.get("file", "?"),
                "line_start": rec.get("lines", [None])[0] if rec.get("lines") else None,
                "line_end": rec.get("lines", [None, None])[1] if rec.get("lines") and len(rec["lines"]) > 1 else None,
                "change": rec.get("change", ""),
                "source_file": rel_path,
                "record_idx": i,
            }
            transcript_groups.setdefault(transcript, []).append(entry)

    # Process each transcript group
    reason_map: dict[int, str] = {}  # edit_id -> reason

    for transcript_path, edits in transcript_groups.items():
        session_prompts = extract_session_prompts(transcript_path)

        if not session_prompts:
            # Fall back to prompts from the records themselves
            seen = set()
            for edit in edits:
                rec = file_records[edit["source_file"]][edit["record_idx"]]
                p = rec.get("prompt", "")
                if p and p not in seen:
                    session_prompts.append(p)
                    seen.add(p)

        prompt = build_fill_prompt(session_prompts, edits)

        if dry_run:
            print(f"\n{BOLD}── Transcript: ...{transcript_path[-60:]}{RESET}")
            print(f"{DIM}{prompt}{RESET}\n")
            continue

        print(f"  Filling {len(edits)} edit(s) from ...{transcript_path[-50:]}",
              file=sys.stderr, end="", flush=True)

        results = call_haiku(prompt)
        if results:
            for item in results:
                eid = item.get("id")
                reason = item.get("reason", "")
                if eid and reason:
                    reason_map[eid] = reason
            print(f" → {len([r for r in results if r.get('reason')])} reasons",
                  file=sys.stderr)
        else:
            print(" → failed", file=sys.stderr)

    # Extract and write trace contexts
    traces_dir = project_root / ".blamebot" / "traces"
    if not dry_run:
        traces_dir.mkdir(parents=True, exist_ok=True)

    for transcript_path, edits in transcript_groups.items():
        # Collect tool_use_ids from records
        tool_use_ids = []
        for edit in edits:
            rec = file_records[edit["source_file"]][edit["record_idx"]]
            trace = rec.get("trace", "")
            if "#" in trace:
                tool_use_ids.append(trace.split("#", 1)[1])

        if not tool_use_ids:
            continue

        contexts = extract_trace_contexts(transcript_path, tool_use_ids)
        if not contexts:
            continue

        # Derive session_id from transcript filename
        session_id = Path(transcript_path).stem

        if dry_run:
            print(f"{DIM}  Traces: {len(contexts)} context(s) for session {session_id[:12]}...{RESET}")
            continue

        # Write/merge traces file
        traces_file = traces_dir / f"{session_id}.json"
        existing = {}
        if traces_file.exists():
            try:
                existing = json.loads(traces_file.read_text())
            except (json.JSONDecodeError, OSError):
                pass
        existing.update(contexts)
        traces_file.write_text(json.dumps(existing, indent=2) + "\n")

        # Stage the traces file
        subprocess.run(
            ["git", "add", str(traces_file.relative_to(project_root))],
            cwd=str(project_root), capture_output=True,
        )

    if dry_run:
        return

    if not reason_map:
        print("No reasons generated.", file=sys.stderr)
        return

    # Patch JSONL files in place
    patched = 0
    for rel_path, records in file_records.items():
        changed = False
        for i, rec in enumerate(records):
            if rec is None:
                continue
            # Find the edit entry for this record
            for edits in transcript_groups.values():
                for edit in edits:
                    if edit["source_file"] == rel_path and edit["record_idx"] == i:
                        if edit["id"] in reason_map:
                            rec["reason"] = reason_map[edit["id"]]
                            changed = True
                            patched += 1

        if changed:
            abs_path = project_root / rel_path
            with open(abs_path, "w") as f:
                for rec in records:
                    if rec is not None:
                        f.write(json.dumps(rec, separators=(",", ":")) + "\n")

            # Re-stage the updated file
            subprocess.run(
                ["git", "add", rel_path],
                cwd=str(project_root), capture_output=True,
            )

    print(f"Filled {patched} reason(s). Index will rebuild on next query.",
          file=sys.stderr)

    # Force index rebuild
    if paths["index_db"].exists():
        paths["index_db"].unlink()


# ── Uninit ────────────────────────────────────────────────────────────

def cmd_uninit(paths: dict, project_root: Path):
    """Remove blamebot tracking from the current repo."""
    removed = []

    # 1. Remove .blamebot/ (committed tracking directory)
    reasons_dir = project_root / ".blamebot"
    if reasons_dir.is_dir():
        shutil.rmtree(reasons_dir)
        removed.append(".blamebot/")

    # 2. Remove .git/blamebot/ (local cache + index)
    cache_dir = paths["cache_dir"]
    if cache_dir.is_dir():
        shutil.rmtree(cache_dir)
        removed.append(".git/blamebot/")

    # 3. Clean pre-commit hook
    pre_commit = project_root / ".git" / "hooks" / "pre-commit"
    if pre_commit.is_file():
        content = pre_commit.read_text()
        marker = "# blamebot: fill reasons"
        if marker in content:
            # Remove the blamebot block
            lines = content.splitlines(keepends=True)
            cleaned = []
            skip = False
            for line in lines:
                if marker in line:
                    skip = True
                    # Also remove a preceding blank line
                    if cleaned and cleaned[-1].strip() == "":
                        cleaned.pop()
                    continue
                if skip:
                    # Skip comment + if/fi block lines that follow the marker
                    stripped = line.strip()
                    if stripped.startswith("#") or stripped.startswith("if ") or \
                       stripped == "git-blamebot --fill-reasons" or stripped == "fi":
                        continue
                    skip = False
                cleaned.append(line)

            remaining = "".join(cleaned).strip()
            if not remaining or remaining == "#!/usr/bin/env bash":
                pre_commit.unlink()
                removed.append(".git/hooks/pre-commit (deleted)")
            else:
                pre_commit.write_text("".join(cleaned))
                removed.append(".git/hooks/pre-commit (cleaned)")

    if removed:
        for item in removed:
            print(f"  Removed {item}")
        print(f"\nblamebot tracking removed from this repo.")
        print(f"Note: the global CLI and hooks are still installed.")
        print(f"Run 'setup.sh init' to re-initialize.")
    else:
        print("blamebot is not initialized in this repo.")


# ── Main ─────────────────────────────────────────────────────────────

def main():
    parser = argparse.ArgumentParser(
        prog="git-blamebot",
        description="Understand why AI-authored code exists.",
    )
    parser.add_argument("file", nargs="?", help="File to inspect")
    parser.add_argument("-L", "--line", help="Line number or range (42 or 10:20)")
    parser.add_argument("--grep", help="Search prompts and changes")
    parser.add_argument("--since", help="Show reasons since date (YYYY-MM-DD)")
    parser.add_argument("--author", help="Filter by author name")
    parser.add_argument("--trace", help="Show reasoning trace for a record ID or tool_use_id")
    parser.add_argument("--explain", action="store_true",
                        help="Deep explanation of an edit using Sonnet (use with <file> -L <line>)")
    parser.add_argument("--stats", action="store_true", help="Summary statistics")
    parser.add_argument("--rebuild", action="store_true", help="Force index rebuild")
    parser.add_argument("--log", action="store_true", help="Show debug logs")
    parser.add_argument("--hook", action="store_true", help="With --log: show hook log")
    parser.add_argument("--dump-payload", action="store_true", help="Show last raw payload")
    parser.add_argument("--fill-reasons", action="store_true",
                        help="Fill empty reasons in staged JSONL files using Claude Haiku")
    parser.add_argument("--dry-run", action="store_true",
                        help="With --fill-reasons: show what would be generated")
    parser.add_argument("--uninit", action="store_true",
                        help="Remove blamebot tracking from this repo")
    parser.add_argument("-v", "--verbose", action="store_true",
                        help="Show hashes, sessions, traces, git blame")
    parser.add_argument("--json", action="store_true",
                        help="Output results as JSON (for programmatic consumers)")

    args = parser.parse_args()

    project_root = get_project_root()
    paths = get_paths(project_root)

    if args.log:
        cmd_log(paths["cache_dir"], args.hook)
        return
    if args.dump_payload:
        cmd_dump_payload(paths["cache_dir"])
        return
    if args.fill_reasons:
        cmd_fill_reasons(paths, project_root, args.dry_run)
        return
    if args.uninit:
        cmd_uninit(paths, project_root)
        return

    if not paths["log_dir"].exists():
        print("No .blamebot/log/ directory found.", file=sys.stderr)
        print("Run 'setup.sh init' in this repo first.", file=sys.stderr)
        sys.exit(1)

    conn = get_db(paths, force_rebuild=args.rebuild)

    if args.stats:
        cmd_stats(conn, json_output=args.json)
    elif args.grep:
        cmd_grep(conn, args.grep, project_root, args.verbose, json_output=args.json)
    elif args.since:
        cmd_since(conn, args.since, args.file, project_root, args.verbose,
                  json_output=args.json)
    elif args.author:
        cmd_author(conn, args.author, project_root, args.verbose, json_output=args.json)
    elif args.trace:
        cmd_trace(conn, args.trace, project_root, json_output=args.json)
    elif args.explain:
        if not args.file:
            print("Usage: git blamebot --explain <file> [-L <line>]", file=sys.stderr)
            sys.exit(1)
        cmd_explain(conn, args.file, project_root, args.line)
    elif args.file:
        cmd_file(conn, args.file, project_root, args.line, args.verbose,
                 json_output=args.json)
    else:
        parser.print_help()

    conn.close()


if __name__ == "__main__":
    main()
